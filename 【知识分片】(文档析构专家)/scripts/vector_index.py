#!/usr/bin/env python3
"""
Lightweight Vector Indexing Tool
è½»é‡åŒ–å‘é‡ç´¢å¼•å·¥å…· - ç”¨äºå¯¹çŸ¥è¯†åˆ†ç‰‡æ‰§è¡Œæ¯«ç§’çº§æ£€ç´¢
ä½¿ç”¨ç®€å•çš„ TF-IDF + Cosine Similarity å®ç°ï¼ˆæ— éœ€å®‰è£…é‡å‹å‘é‡åº“ï¼‰
"""

import json
import os
from pathlib import Path
from math import log, sqrt
from collections import Counter

class SimpleVectorStore:
    def __init__(self, shards_dir):
        self.shards_dir = Path(shards_dir)
        self.index_file = self.shards_dir / "vector_index.json"
        self.shards = []
        self.vocab = {}
        self.vectors = []

    def _tokenize(self, text):
        # æç®€åˆ†è¯ï¼šå°å†™åŒ–å¹¶æå–å­—æ¯æ•°å­—å­—ç¬¦
        import re
        return re.findall(r'\w+', text.lower())

    def build_index(self):
        """æ‰«ææ‰€æœ‰åˆ†ç‰‡å¹¶å»ºç«‹ TF-IDF ç´¢å¼•"""
        print(f"ğŸ” æ­£åœ¨ä» {self.shards_dir} æ„å»ºå‘é‡ç´¢å¼•...")
        
        all_shards = list(self.shards_dir.glob("**/*.md")) + list(self.shards_dir.glob("**/*.json"))
        if not all_shards:
            print("âš ï¸ æœªå‘ç°å¯åˆ†ç‰‡æ–‡ä»¶ã€‚")
            return

        documents = []
        for p in all_shards:
            try:
                content = p.read_text(encoding='utf-8')
                documents.append({"path": str(p), "content": content})
            except:
                continue

        # è®¡ç®— TF-IDF
        num_docs = len(documents)
        df = Counter()
        doc_tfs = []

        for doc in documents:
            tokens = self._tokenize(doc['content'])
            tf = Counter(tokens)
            doc_tfs.append(tf)
            for word in tf:
                df[word] += 1

        # æ„å»ºè¯æ±‡è¡¨å’Œå‘é‡
        self.vocab = {word: i for i, word in enumerate(df)}
        self.vectors = []
        
        for doc_idx, tf in enumerate(doc_tfs):
            vector = {}
            for word, count in tf.items():
                idf = log(num_docs / (df[word] + 1))
                vector[self.vocab[word]] = count * idf
            self.vectors.append({"path": documents[doc_idx]['path'], "vector": vector})

        # æŒä¹…åŒ–
        self.save()
        print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…±å¤„ç† {num_docs} ä¸ªæ–‡æ¡£ã€‚")

    def save(self):
        data = {
            "vocab": self.vocab,
            "vectors": self.vectors
        }
        self.index_file.write_text(json.dumps(data, indent=2))

    def load(self):
        if not self.index_file.exists():
            self.build_index()
        data = json.loads(self.index_file.read_text())
        self.vocab = data['vocab']
        self.vectors = data['vectors']

    def search(self, query, top_k=3):
        """å¯¹æ¯”ä½™å¼¦ç›¸ä¼¼åº¦è¿›è¡Œå†…å®¹æ£€ç´¢"""
        if not self.vocab:
            self.load()

        query_tokens = self._tokenize(query)
        query_tf = Counter(query_tokens)
        query_vec = {}
        
        for word, count in query_tf.items():
            if word in self.vocab:
                query_vec[self.vocab[word]] = count

        results = []
        for doc in self.vectors:
            score = self._cosine_similarity(query_vec, doc['vector'])
            if score > 0:
                results.append({"path": doc['path'], "score": score})

        results.sort(key=lambda x: x['score'], reverse=True)
        return results[:top_k]

    def _cosine_similarity(self, vec1, vec2):
        intersection = set(vec1.keys()) & set(vec2.keys())
        numerator = sum([vec1[x] * vec2[x] for x in intersection])

        sum1 = sum([val**2 for val in vec1.values()])
        sum2 = sum([val**2 for val in vec2.values()])
        denominator = sqrt(sum1) * sqrt(sum2)

        if not denominator:
            return 0.0
        return float(numerator) / denominator

if __name__ == "__main__":
    # é»˜è®¤é’ˆå¯¹çŸ¥è¯†åˆ†ç‰‡ç›®å½•
    store = SimpleVectorStore("ã€çŸ¥è¯†åˆ†ç‰‡ã€‘(æ–‡æ¡£ææ„ä¸“å®¶)")
    store.build_index()
    # ç¤ºä¾‹æœç´¢
    # print(store.search("äº¤æ˜“ç­–ç•¥"))
